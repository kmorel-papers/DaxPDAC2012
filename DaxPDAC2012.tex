% -*- latex -*-

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This beginning part of the preamble is specific to the
% acm document class.

\documentclass{sig-alternate}


%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

%\numberofauthors{12}
\numberofauthors{1}

% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.

% Note, I am overriding affaddr and email commands because I don't like the
% font sizes used.
\newcommand*{\myaffaddr}[1]{\affaddr{\normalsize #1}}
\newcommand*{\myemail}[1]{\email{\normalsize #1}}

\author{
  %
  \alignauthor Kenneth Moreland \\
  \myaffaddr{Sandia National Laboratories} \\
  \myemail{kmorel@sandia.gov}
  %% %
  %% \alignauthor Ron Oldfield \\
  %% \myaffaddr{Sandia National Laboratories} \\
  %% \myemail{raoldfi@sandia.gov}
  %% %
  %% \alignauthor Pat Marion \\
  %% \myaffaddr{Kitware, Inc.} \\
  %% \myemail{pat.marion@kitware.com}
  %% %
  %% \and % Another row of authors.
  %% %
  %% \alignauthor Sebastien Jourdain \\
  %% \myaffaddr{Kitware, Inc.} \\
  %% \myemail{sebastien.jourdain@kitware.com}
  %% %
  %% \alignauthor Norbert Podhorszki \\
  %% \myaffaddr{Oak Ridge National Laboratory} \\
  %% \myemail{pnorbert@ornl.gov}
  %% %
  %% \alignauthor Venkatram Vishwanath \\
  %% \myaffaddr{Argonne National Laboratory} \\
  %% \myemail{venkatv@mcs.anl.gov}
  %% %
  %% \and
  %% %
  %% \alignauthor Nathan Fabian \\
  %% \myaffaddr{Sandia National Laboratories} \\
  %% \myemail{ndfabia@sandia.gov}
  %% %
  %% \alignauthor Ciprian Docan \\
  %% \myaffaddr{Rutgers University} \\
  %% \myemail{docan@caip.rutgers.edu}
  %% %
  %% \alignauthor Manish Parashar \\
  %% \myaffaddr{Rutgers University} \\
  %% \myemail{parashar@rutgers.edu}
  %% %
  %% \and
  %% %
  %% \alignauthor Mark Hereld \\
  %% \myaffaddr{Argonne National Laboratory} \\
  %% \myemail{hereld@mcs.anl.gov}
  %% %
  %% \alignauthor Michael E. Papka \\
  %% \myaffaddr{Argonne National Laboratory} \\
  %% \myemail{papka@anl.gov}
  %% %
  %% \alignauthor Scott Klasky \\
  %% \myaffaddr{Oak Ridge National Laboratory} \\
  %% \myemail{klasky@ornl.gov}
}

%% \author{
%% \alignauthor
%% Kenneth~Moreland,$^{\ddagger}$ Ron~Oldfield,$^{\ddagger}$ Pat~Marion,$^{*}$ Sebastien~Jourdain,$^{*}$ Norbert~Podhorszki,$^{\dagger}$ Venkatram~Vishwanath,$^{\P}$ Nathan~Fabian,$^{\ddagger}$ Ciprian~Docan,$^{\S}$ Manish~Parashar,$^{\S}$ Mark~Hereld,$^{\P}$ Michael~E.~Papka,$^{\P}$ and Scott~Klasky$^{\dagger}$\\
%%     \affaddr{$^\ddagger$Sandia National Laboratories}\\
%%     \affaddr{$^*$Kitware, Inc.}\\
%%     \affaddr{$^\dagger$Oak Ridge National Laboratory}\\
%%     \affaddr{$^\P$Argonne National Laboratory}\\
%%     \affaddr{$^\S$Rutgers University}
%% }

\conferenceinfo{PDAC'12,} {November 12, 2012, Salt Lake City, Utah, USA.}
\CopyrightYear{2012}
%% \crdata{978-1-4503-1130-4/11/11}
\clubpenalty=10000
\widowpenalty = 10000

% End of acm-specific portion of the preamble.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{varioref}
\usepackage{fancyvrb}
\usepackage{ifthen}
\usepackage{cite}
\usepackage{subfig}
\usepackage{xspace}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{verbatim}

\usepackage{color}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{ltcyan}{rgb}{.75,1,1}
\definecolor{red}{rgb}{1,0,0}

% Cite commands I use to abstract away the different ways to reference an
% entry in the bibliography (superscripts, numbers, dates, or author
% abbreviations).  \scite is a short cite that is used immediately after
% when the authors are mentioned.  \lcite is a full citation that is used
% anywhere.  Both should be used right next to the text being cited without
% any spacing.
\newcommand*{\lcite}[1]{~\cite{#1}}
\newcommand*{\scite}[1]{~\cite{#1}}

\newcommand{\etal}{et al.}

\newcommand*{\keyterm}[1]{\emph{#1}}

\newcommand{\fix}[1]{{\color{red}\textsc{[#1]}}}

% Avoid putting figures on their own page.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}

% Make sure this is big enough so that only big figures end up on their own
% page but small enough so that if a figure does have to be on its own
% page, it won't push everything to the bottom because it's not big enough
% to have its own page.
\renewcommand{\floatpagefraction}{.75}

\newenvironment{packed_itemize}{
\begin{itemize}
  \setlength{\topsep}{0pt}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
  \setlength{\partopsep}{0pt}
}{\end{itemize}}

\title{Flexible Analysis Software for Emerging Architectures}

\hyphenation{Para-View Map-Re-duce}

\begin{document}

\sloppy

\maketitle

\begin{abstract}
  We are on the threshold of a transformative change in the basic
  architecture of high-performance computing.  The use of accelerator
  processors, characterized by large core counts, shared but asymmetrical
  memory, and heavy thread loading, is quickly becoming the norm in high
  performance computing.  These accelerators represent significant
  challenges in updating our existing base of software.  An intrinsic
  problem with this transition is a fundamental programming shift from
  message passing processes to much more fine thread scheduling with memory
  sharing.  Another problem is the lack of stability in accelerator
  implementation; processor and compiler technology is currently changing
  rapidly.  In this paper we describe our approach to address these two
  immediate problems with respect to scientific analysis and visualization
  algorithms.  Our approach to accelerator programming forms the basis of
  the Dax toolkit, a framework to build data analysis and visualization
  algorithms applicable to exascale computing.
\end{abstract}

\section{Introduction}
\label{sec:Introduction}

\noindent
Whereas supercomputers throughout the terascale era were almost
unilaterally built from general purpose CPU processors on distributed
memory nodes with a message passing interface, with petascale computing we
are seeing the emerging use of accelerators to meet the execution and
computation requirements of modern leadership-class facilities.  This trend
was kicked off when the Roadrunner supercomputer, first to achieve a
petaFLOP, was build with Cell BE processors\fix{cite}.  At the time,
Roadrunner was an anomaly, but since then many high-performance computers
followed this example.  Today, \fix{X} out of 10 of the fastest supercomputers
are also built with accelerator technology\fix{cite Top500?}.

These accelerators represent a significant departure from how we most often
perform parallel processing.  Computing on the previous generation of high
performance computers involved partitioning data among distributed memory
nodes and running independent processes that pass messages.  However,
accelerators do not work well with such an approach.  Threads on an
accelerator may be grouped in SIMD ``warps,'' can have indeterminate
scheduling, and may be incapable of direct message passing\fix{citeCUDA?}.
Even on processors with more complete and independent cores, taking
advantage of shared memory threads can have its
advantages\lcite{Camp2010,Howison2011}.  Ultimately, our algorithms must
exhibit a more ``pervasive parallelism'' comprising a marked increase in
concurrency and careful data
management\lcite{VisAnalysisExtremeScale,ExascaleRoadMap}. \fix{Check the
  VisAnalysisExtremeScale reference (and similar
  VisualizationandKnowledgeDiscovery in main collection).  I seem to be
  mixing this up with Sean Ahern's report.}

Another problem facing current research and development is the shifting
landscape of the development environment.  The Cell BE processors (and
associated compiler environment) comprising Roadrunner is already
discontinued.  Instead, NVIDIA is aggressively pursing leadership in
accelerator technology for scientific computation with Intel hot on its
heels.  Several compiler technologies such as OpenMP, CUDA, Threaded
Building Blocks, and OpenACC \fix{cite} also compete for multi-threaded
programming.

Our team is creating the Dax toolkit\lcite{Moreland2011:LDAV}, which seeks
to provide a development framework for scientific data analysis and
visualization algorithms for the next generation of high-performance
computers and beyond.  In this paper we document the following features.
\begin{itemize}
\item A general approach to data analysis and visualization algorithm
  development that provides a pervasive parallelism without the complexity
  of parallel programming.
\item An adapter mechanism that encapsulates the change in behavior
  required port the toolkit among devices and compilers.
\end{itemize}

\section{Previous Work}
\label{sec:PreviousWork}

\noindent
To implement algorithms that are configurable with respect to operations,
data structures, and processor idiosyncrasies, Dax relies on
well-established techniques of generic programming\fix{cite}.  Generic
programming uses C++ templates to direct the compiler to specialize a
particular piece of code to alternate implementations.

To maximize the amount of code that has no parallel dependencies, Dax
employs a functor-based execution mode\fix{Cite Heurox or go away?}.  The
intention of this approach is to write a sequential section of code that
operates on a small section of data as a functional object, and then
schedule this function in parallel independently on large vector
components.  The technique can be thought of as a generalization of the map
and reduce operations in a MapReduce\lcite{MapReduce} framework.

A toolkit with similar goals of simplifying many-core parallel programming
and cross-device porting is Thrust\lcite{Thrust}.  Thrust is a more general
template library that provides a number of generic parallel algorithms.
Thrust provides many of the desired attributes of Dax and is in fact used
to implement many of them.  What differentiates Dax is the simplification
and specialization of its interface.  We can provide generic algorithms and
classes designed specifically for data analysis and visualization as well
as better specialize the data management.

It should be noted that this paper does not cover message passing,
distributed memory, or ``hybrid'' parallelism.  Although this is clearly
important in high-performance computing, the scope of this paper is only on
the shared-memory, many-core parallelism part of this problem.  The
techniques discussed here can be coupled with existing distributed memory
approaches \fix{Ahrens2000, DIY} to complete the hybrid parallelism
required to run concurrently across an entire machine.

\section{Algorithmic Approach}

\section{Device Adapter}

\section{Generic Array Handle}

\bibliographystyle{abbrv}
\bibliography{DaxPDAC2012}

\end{document}
