%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for kmorel at 2012-09-28 09:58:26 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@article{Blondin2003,
	Abstract = {We examine the stability of standing, spherical accretion shocks. Accretion shocks arise in core-collapse supernovae (the focus of this paper), star formation, and accreting white dwarfs and neutron stars. We present a simple analytic model and use time-dependent hydrodynamics simulations to show that this solution is stable to radial perturbations. In two dimensions we show that small perturbations to a spherical shock front can lead to rapid growth of turbulence behind the shock, driven by the injection of vorticity from the now nonspherical shock. We discuss the ramifications this instability may have for the supernova mechanism.},
	Annote = {This is the proper reference to the supernova data set we use for a lot of testing.},
	Author = {John M. Blondin and Anthony Mezzacappa and Christine DeMarino},
	Date-Added = {2012-09-28 09:54:30 -0600},
	Date-Modified = {2012-09-28 09:58:11 -0600},
	Doi = {10.1086/345812},
	Journal = {The Astrophysical Journal},
	Month = {February},
	Note = {{DOI}~10.1086/345812},
	Number = {2},
	Pages = {971--980},
	Title = {Stability of Standing Accretion Shocks, with an Eye toward Core-Collapse Supernovae },
	Url = {http://iopscience.iop.org/0004-637X/584/2/971/},
	Volume = {584},
	Year = {2003}}

@book{Musser1996,
	Annote = {Refernce book for the C++ STL library.  I have also used this as a reference to generic programming.},
	Author = {David R. Musser and Atul Saini},
	Date-Added = {2012-09-28 09:22:58 -0600},
	Date-Modified = {2012-09-28 09:22:58 -0600},
	Note = {{ISBN}~0-201-63398-1},
	Publisher = {Addison Wesley},
	Title = {{STL} Tutorial and Reference Guide},
	Year = {1996}}

@book{Sanders2011,
	Annote = {A book describing how to use CUDA including a lot of practical advice.},
	Author = {Jason Sanders and Edward Kandrot},
	Date-Added = {2012-09-28 09:17:11 -0600},
	Date-Modified = {2012-09-28 09:17:11 -0600},
	Note = {{ISBN}~978-0-13-138768-3},
	Publisher = {Addison Wesley},
	Title = {{CUDA} by Example},
	Year = {2011}}

@book{Blelloch1990,
	Abstract = {Vector Models for Data-Parallel Computing describes a model of parallelism that extends and formalizes the Data-Parallel model on which the Connection Machine and other supercomputers are based. It presents many algorithms based on the model, ranging from graph algorithms to numerical algorithms, and argues that data-parallel models are not only practical and can be applied to a surprisingly wide variety of problems, they are also well suited for very-high-level languages and lead to a concise and clear description of algorithms and their complexity. Many of the author's ideas have been incorporated into the instruction set and into algorithms currently running on the Connection Machine.

The book includes the definition of a parallel vector machine; an extensive description of the uses of the scan (also called parallel-prefix) operations; the introduction of segmented vector operations; parallel data structures for trees, graphs, and grids; many parallel computational-geometry, graph, numerical and sorting algorithms; techniques for compiling nested parallelism; a compiler for Paralation Lisp; and details on the implementation of the scan operations.},
	Annote = {Seminal work on using scans and other basic parallel algorithms to build more specific data-parallel algorithms.},
	Author = {Guy E. Blelloch},
	Date-Added = {2012-09-25 16:01:39 -0600},
	Date-Modified = {2012-09-25 16:01:39 -0600},
	Note = {{ISBN}~0-262-02313-X},
	Publisher = {MIT Press},
	Title = {Vector Models for Data-Parallel Computing},
	Year = {1990}}

@inproceedings{Peterka2011,
	Abstract = {We present a set of building blocks that provide scalable data movement capability to computational scientists and visualization researchers for writing their own parallel analysis. The set includes scalable tools for domain decomposition, process assignment, parallel I/O, global reduction, and local neighborhood communicationtasks that are common across many analysis applications. The global reduction is performed with a new algorithm, described in this paper, that efficiently merges blocks of analysis results into a smaller number of larger blocks. The merging is configurable in the number of blocks that are reduced in each round, the number of rounds, and the total number of resulting blocks. We highlight the use of our library in two analysis applications: parallel streamline generation and parallel Morse-Smale topological analysis. The first case uses an existing local neighborhood communication algorithm, whereas the latter uses the new merge algorithm.},
	Annote = {Description of the Do-It-Yourself (DIY) library for building distributed-memory parallel scientific visualizaiton algorithms.},
	Author = {Tom Peterka and Robert Ross and Wesley Kendall and Attila Gyulassy and Valerio Pascucci and Han-Wei Shen and Teng-Yok Lee and Abon Chaudhuri},
	Booktitle = {Proceedings of Large Data Analysis and Visualization Symposium LDAV'11},
	Date-Added = {2012-09-25 15:39:59 -0600},
	Date-Modified = {2012-09-25 15:39:59 -0600},
	Doi = {10.1109/LDAV.2011.6092324},
	Month = {October},
	Note = {{DOI}~10.1109/LDAV.2011.6092324},
	Pages = {105--112},
	Title = {Scalable Parallel Building Blocks for Custom Data Analysis},
	Url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6092324},
	Year = {2011},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6092324},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/LDAV.2011.6092324}}

@misc{ScientificDiscoveryExascale2011,
	Annote = {Workshop report on upcoming visualization challenges at exascale.  Big challenges include overcoming lower relative I/O rates through various more advanced I/O technologies such as in situ computations.  Also includes discussion on upcoming algorithmic challenges.},
	Author = {Sean Ahern and Arie Shoshani and Kwan-Liu Ma and others},
	Date-Added = {2012-09-25 15:24:30 -0600},
	Date-Modified = {2012-09-25 15:24:30 -0600},
	Howpublished = {Report from the DOE ASCR 2011 Workshop on Exascale Data Management, Analysis, and Visualization},
	Month = {February},
	Title = {Scientific Discovery at the Exascale},
	Year = {2011}}

@article{Henning2009,
	Abstract = {In June 2008, a new supercomputer broke the petaflop/s performance barrier, more than doubling the computational performance of the next fastest machine on the Top500 Supercomputer list. This computer, named Roadrunner, is the result of an intensive collaboration between IBM and Los Alamos National Laboratory, where it's now located. Aside from its performance, Roadrunner has two distinguishing characteristics: a very good power/performance ratio and a ``hybrid'' computer architecture that mixes several types of processors. By November 2008, the traditionally architected Jaguar computer at Oak Ridge National Laboratory was tied with Roadrunner in the performance race, but it requires almost 2.8 times the electric power of Roadrunner. This difference translates into millions of dollars per year in operating costs.},
	Annote = {A high-level article introducing the roadrunner petaflop supercomputer at Los Alamos National Laboraotry (LANL).  It describes the basic architecture of the nodes including the use of the IBM Cell BE (Broadband Engine).},
	Author = {Paul Henning and Andrew B. {White Jr.}},
	Date-Added = {2012-09-25 14:51:30 -0600},
	Date-Modified = {2012-09-25 14:51:30 -0600},
	Journal = {Computing in Science \& Engineering},
	Month = {July/August},
	Pages = {91--95},
	Title = {Trailblazing with {Roadrunner}},
	Year = {2009}}

@inproceedings{Baker2010,
	Abstract = {Multicore nodes have become ubiquitous in just a few years. At the same time, writing portable parallel software for multicore nodes is extremely challenging. Widely available programming models such as OpenMP and Pthreads are not useful for devices such as graphics cards, and more flexible programming models such as RapidMind are only available commercially. OpenCL represents the first truly portable standard, but its availability is limited. In the presence of such transition, we have developed a minimal application programming interface (API) for multicore nodes that allows us to write portable parallel linear algebra software that can use any of the aforementioned programming models and any future standard models. We utilize C++ template meta-programming to enable users to write parallel kernels that can be executed on a variety of node types, including Cell, GPUs and multicore CPUs. The support for a parallel node is provided by implementing a Node object, according to the requirements specified by the API. This ability to provide custom support for particular node types gives developers a level of control not allowed by the current slate of proprietary parallel programming APIs. We demonstrate implementations of the API for a simple vector dot-product on sequential CPU, multicore CPU and GPU nodes.},
	Annote = {This is a paper from Mike Heroux and gang about the multi-core parallel technique using functors.  It makes for a vary portable system parallel system and this is demonstrated on intel threaded building blocks (TBB) and CUDA.},
	Author = {Christopher G. Baker and Michael A. Heroux and H. Carter Edwards and Alan B. Williams},
	Booktitle = {Proceedings of the 18th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
	Date-Added = {2012-09-14 22:44:33 -0600},
	Date-Modified = {2012-09-14 22:44:33 -0600},
	Doi = {10.1109/PDP.2010.49},
	Month = {February},
	Note = {{DOI}~10.1109/PDP.2010.49},
	Pages = {601 -- 606},
	Title = {A Light-weight {API} for Portable Multicore Programming},
	Url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5452412},
	Year = {2010},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5452412},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/PDP.2010.49}}

@techreport{Ahrens2000,
	Annote = {One of the seminal papers on introducing parallelism and data streaming into VTK specifically and into pipelines in general.},
	Author = {James Ahrens and Charles Law and Will Schroeder and Ken Martin and Michael Papka},
	Date-Added = {2012-09-12 19:19:11 -0600},
	Date-Modified = {2012-09-12 19:19:11 -0600},
	Institution = {Los Alamos National Laboratory},
	Number = {\#LAUR-00-1620},
	Title = {A Parallel Approach for Efficiently Visualizing Extremely Large, Time-Varying Datasets},
	Year = {2000}}

@inbook{Thrust,
	Annote = {A portable STL-like template library for use on multi- and many-core systems.},
	Author = {Nathan Bell and Jared Hoberock},
	Chapter = {Thrust: A Productivity-Oriented Library for {CUDA}},
	Date-Added = {2012-09-12 19:08:38 -0600},
	Date-Modified = {2012-09-12 19:08:38 -0600},
	Month = {October},
	Pages = {359--371},
	Publisher = {Morgan Kaufmann},
	Title = {GPU Computing Gems, Jade Edition},
	Year = {2011}}

@article{MapReduce,
	Annote = {The seminal MapReduce paper.},
	Author = {Jeffrey Dean and Sanjay Ghemawat},
	Date-Added = {2012-09-12 19:03:21 -0600},
	Date-Modified = {2012-09-12 19:03:21 -0600},
	Journal = {Communications of the ACM},
	Month = {January},
	Number = {1},
	Pages = {107--113},
	Title = {{MapReduce}: Simplified Data Processing on Large Clusters},
	Volume = {51},
	Year = {2008}}

@inproceedings{Moreland2011:LDAV,
	Annote = {Description of the Dax toolkit, a framework for building algorithms for GPU computers and, later, exascale computers.},
	Author = {Kenneth Moreland and Utkarsh Ayachit and Berk Geveci and Kwan-Liu Ma},
	Booktitle = {Proceedings of the IEEE Symposium on Large-Scale Data Analysis and Visualization},
	Date-Added = {2012-09-12 18:40:43 -0600},
	Date-Modified = {2012-09-12 18:40:43 -0600},
	Doi = {10.1109/LDAV.2011.6092323},
	Month = {October},
	Note = {{DOI}~10.1109/LDAV.2011.6092323},
	Pages = {97--104},
	Title = {Dax Toolkit: A Proposed Framework for Data Analysis and Visualization at Extreme Scale},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/LDAV.2011.6092323}}

@techreport{VisAnalysisExtremeScale,
	Author = {Chris Johnson and Rob Ross},
	Date-Added = {2012-09-12 18:01:48 -0600},
	Date-Modified = {2012-09-12 18:01:48 -0600},
	Month = {October},
	Title = {Visualization and Knowledge Discovery: Report from the {DOE}/{ASCR} Workshop on Visual Analysis and Data Exploration at Extreme Scale},
	Year = {2007}}

@techreport{ExascaleRoadMap,
	Annote = {A workshop report prediciting the nature of an Exascale machine.  A reasonable source to pull some numbers comparing petascale to exascale machines (a la slide of doom).  See [Moreland2011:LDAV] for a usage.},
	Author = {Jack Dongarra and Pete Beechman and others},
	Date-Added = {2012-09-12 17:57:28 -0600},
	Date-Modified = {2012-09-12 17:57:28 -0600},
	Institution = {University of Tennessee},
	Month = {January},
	Number = {ut-cs-10-652},
	Title = {The International Exascale Software Project RoadMap},
	Url = {http://www.cs.utk.edu/~library/TechReports/2010/ut-cs-10-652.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://www.cs.utk.edu/~library/TechReports/2010/ut-cs-10-652.pdf}}

@article{Howison2011,
	Annote = {Describes a system for using hybrid MPI+OpenMP/threaded parallelism for improving volume rendering.  Predictably, the hybrid method is faster than MPI-only.  (I, however, feel the comparison was performed unfairly because they used a crappy image composition algorithm.)  They also demonstrate compositing with a very large number of cores (216K), but only by using an MPI_Reduce.},
	Author = {Mark Howison and E. Wes Bethel and Hank Childs},
	Date-Added = {2012-09-12 19:44:22 -0400},
	Date-Modified = {2012-09-12 19:44:22 -0400},
	Doi = {10.1109/TVCG.2011.24},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {January},
	Note = {{DOI}~10.1109/TVCG.2011.24},
	Number = {1},
	Pages = {17--29},
	Title = {Hybrid Parallelism for Volume Rendering on Large-, Multi- and Many-core Systems},
	Volume = {18},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2011.24}}

@article{Camp2010,
	Annote = {An example of hybrid MPI/OpenMP (thread) parallelism.  Uses shared memory to allow streamline integration to progress further before having to load a new data block or pass the streamline to another process.  This is an extension of [Pugmire2009].},
	Author = {David Camp and Christoph Garth and Hank Childs and David Pugmire and Kenneth Joy},
	Date-Added = {2012-09-12 19:43:46 -0400},
	Date-Modified = {2012-09-12 19:43:46 -0400},
	Doi = {10.1109/TVCG.2010.259},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {December},
	Note = {{DOI}~10.1109/TVCG.2010.259},
	Title = {Streamline Integration using {MPI}-Hybrid Parallelism on Large Multi-Core Architecture},
	Year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2010.259}}
